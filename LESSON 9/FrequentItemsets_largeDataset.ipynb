{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Frequent Itemset Mining & Association Rules with CSV Dataset\n"
      ],
      "metadata": {
        "id": "xHdQmHKv7Fn2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3rt-ZW8D7Abb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "\n",
        "def load_transactions(csv_file):\n",
        "    \"\"\"Load transactions from a CSV file\"\"\"\n",
        "    df = pd.read_csv(csv_file)\n",
        "    transactions = []\n",
        "    for index, row in df.iterrows():\n",
        "        transaction = set(row.dropna().values)  # Remove NaN values\n",
        "        transactions.append(transaction)\n",
        "    return transactions\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_candidates(frequent_itemsets, k):\n",
        "    \"\"\"Generate candidate itemsets of size k from frequent itemsets of size k-1\"\"\"\n",
        "    candidates = set()\n",
        "    frequent_items = list(frequent_itemsets.keys())\n",
        "    for i in range(len(frequent_items)):\n",
        "        for j in range(i+1, len(frequent_items)):\n",
        "            union_set = frequent_items[i] | frequent_items[j]\n",
        "            if len(union_set) == k:\n",
        "                candidates.add(frozenset(union_set))\n",
        "    return candidates\n",
        "\n",
        "def get_frequent_itemsets(transactions, candidates, min_support):\n",
        "    \"\"\"Filter candidates to get frequent itemsets based on support threshold\"\"\"\n",
        "    itemset_counts = {}\n",
        "    for transaction in transactions:\n",
        "        for candidate in candidates:\n",
        "            if candidate.issubset(transaction):\n",
        "                itemset_counts[candidate] = itemset_counts.get(candidate, 0) + 1\n",
        "    total_transactions = len(transactions)\n",
        "    return {itemset: count / total_transactions for itemset, count in itemset_counts.items() if count / total_transactions >= min_support}\n",
        "\n",
        "def apriori(transactions, min_support):\n",
        "    \"\"\"Apriori Algorithm to mine frequent itemsets\"\"\"\n",
        "    single_items = {frozenset([item]) for transaction in transactions for item in transaction}\n",
        "    frequent_itemsets = get_frequent_itemsets(transactions, single_items, min_support)\n",
        "    all_frequent_itemsets = frequent_itemsets.copy()\n",
        "    k = 2\n",
        "    while frequent_itemsets:\n",
        "        candidates = generate_candidates(frequent_itemsets, k)\n",
        "        frequent_itemsets = get_frequent_itemsets(transactions, candidates, min_support)\n",
        "        all_frequent_itemsets.update(frequent_itemsets)\n",
        "        k += 1\n",
        "    return all_frequent_itemsets"
      ],
      "metadata": {
        "id": "1lzsg20g7LAj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_association_rules(frequent_itemsets, min_confidence):\n",
        "    \"\"\"Generate association rules from frequent itemsets\"\"\"\n",
        "    rules = []\n",
        "    for itemset in frequent_itemsets:\n",
        "        if len(itemset) > 1:\n",
        "            for i in range(1, len(itemset)):\n",
        "                for antecedent in combinations(itemset, i):\n",
        "                    antecedent = frozenset(antecedent)\n",
        "                    consequent = itemset - antecedent\n",
        "                    support_itemset = frequent_itemsets[itemset]\n",
        "                    support_antecedent = frequent_itemsets[antecedent]\n",
        "                    confidence = support_itemset / support_antecedent\n",
        "                    if confidence >= min_confidence:\n",
        "                        rules.append((antecedent, consequent, confidence))\n",
        "    return rules"
      ],
      "metadata": {
        "id": "vQQMdMOR7OAe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset from CSV file\n",
        "csv_file = \"transactions.csv\"\n",
        "transactions = load_transactions(csv_file)\n",
        "print(f\"Loaded {len(transactions)} transactions from {csv_file}\")\n",
        "\n",
        "# set support and confidence thresholds\n",
        "min_support = 0.09\n",
        "min_confidence = 0.30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_Jwhoj07QDC",
        "outputId": "24bc0def-eef0-427b-bb30-bc83859550bf"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 499 transactions from transactions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1: mine frequent itemsets\n",
        "frequent_itemsets = apriori(transactions, min_support)\n",
        "print(\"Frequent Itemsets:\")\n",
        "for itemset, support in frequent_itemsets.items():\n",
        "    print(f\"{set(itemset)}: {support:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8gQFxXn7TX5",
        "outputId": "47a30a35-58b8-48e5-b88a-9d6bbd11a333"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequent Itemsets:\n",
            "{'Bread'}: 0.26\n",
            "{'Fish'}: 0.27\n",
            "{'Juice'}: 0.30\n",
            "{'Butter'}: 0.28\n",
            "{'Eggs'}: 0.30\n",
            "{'Cereal'}: 0.31\n",
            "{'Tomato'}: 0.30\n",
            "{'Chicken'}: 0.23\n",
            "{'Milk'}: 0.27\n",
            "{'Beer'}: 0.23\n",
            "{'Pasta'}: 0.29\n",
            "{'Cheese'}: 0.26\n",
            "{'Yogurt'}: 0.29\n",
            "{'Diapers'}: 0.26\n",
            "{'Rice'}: 0.27\n",
            "{'Juice', 'Eggs'}: 0.09\n",
            "{'Bread', 'Eggs'}: 0.09\n",
            "{'Cereal', 'Eggs'}: 0.10\n",
            "{'Bread', 'Tomato'}: 0.09\n",
            "{'Tomato', 'Juice'}: 0.10\n",
            "{'Tomato', 'Yogurt'}: 0.09\n",
            "{'Fish', 'Yogurt'}: 0.09\n",
            "{'Butter', 'Tomato'}: 0.10\n",
            "{'Yogurt', 'Cereal'}: 0.09\n",
            "{'Yogurt', 'Pasta'}: 0.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Generate Association Rules\n",
        "association_rules = generate_association_rules(frequent_itemsets, min_confidence)\n",
        "print(\"\\nAssociation Rules:\")\n",
        "for antecedent, consequent, confidence in association_rules:\n",
        "    print(f\"{set(antecedent)} -> {set(consequent)} (Confidence: {confidence:.2f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DztIW2iT7VVj",
        "outputId": "c7096d98-9e43-454e-ea42-e5f78bb6c06a"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Association Rules:\n",
            "{'Juice'} -> {'Eggs'} (Confidence: 0.30)\n",
            "{'Eggs'} -> {'Juice'} (Confidence: 0.31)\n",
            "{'Bread'} -> {'Eggs'} (Confidence: 0.34)\n",
            "{'Eggs'} -> {'Bread'} (Confidence: 0.30)\n",
            "{'Cereal'} -> {'Eggs'} (Confidence: 0.31)\n",
            "{'Eggs'} -> {'Cereal'} (Confidence: 0.33)\n",
            "{'Bread'} -> {'Tomato'} (Confidence: 0.36)\n",
            "{'Tomato'} -> {'Bread'} (Confidence: 0.31)\n",
            "{'Tomato'} -> {'Juice'} (Confidence: 0.32)\n",
            "{'Juice'} -> {'Tomato'} (Confidence: 0.32)\n",
            "{'Yogurt'} -> {'Tomato'} (Confidence: 0.31)\n",
            "{'Fish'} -> {'Yogurt'} (Confidence: 0.33)\n",
            "{'Yogurt'} -> {'Fish'} (Confidence: 0.31)\n",
            "{'Butter'} -> {'Tomato'} (Confidence: 0.35)\n",
            "{'Tomato'} -> {'Butter'} (Confidence: 0.32)\n",
            "{'Yogurt'} -> {'Cereal'} (Confidence: 0.31)\n",
            "{'Yogurt'} -> {'Pasta'} (Confidence: 0.34)\n",
            "{'Pasta'} -> {'Yogurt'} (Confidence: 0.34)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Class Activity Questions:**\n",
        " 1. Run the script with a large dataset. How do frequent itemsets change as dataset size increases?\n",
        "\n",
        "  Off the bat, there were no frequent itemsets or association rules with support threshold 0.4 and confidence 0.75. Lowering the thresholds to support 0.3 identified 3 single item frequent itemsets  of  \n",
        "- {'Juice'}: 0.30\n",
        "- {'Cereal'}: 0.31\n",
        "- {'Tomato'}: 0.30\n",
        "\n",
        "Lowering the threshold to 0.2 idenitifed further single item frequent itemset,s but no multi-item frequent itemsets. In order to get 2-item frequent item sets, i have to lower the threshold to 0.05.\n",
        "\n",
        "so, overall we can say that larger datasets have a more diverse set of items, which can reduce overall support for any single item. Multi-item frequent sets need lower support thresholds because combinations of items naturally occur less frequently than single items. In real-world applications, support thresholds need careful tuning to balance relevance vs. noise.\n",
        "\n",
        "\n",
        " 2. Modify support and confidence thresholds. What changes do you observe in the output?\n",
        " See above.\n",
        " For confidence thresholds, at 0.75- there are no associations. After playing with both the support threshold and confidence threshold, i was able to achieve some  association rules when using support threshold of 0.09 and confidence threshold of 0.3\n",
        " Association Rules:\n",
        "- {'Juice'} -> {'Eggs'} (Confidence: 0.30)\n",
        "- {'Eggs'} -> {'Juice'} (Confidence: 0.31)\n",
        "- {'Bread'} -> {'Eggs'} (Confidence: 0.34)\n",
        "- {'Eggs'} -> {'Bread'} (Confidence: 0.30)\n",
        "- {'Cereal'} -> {'Eggs'} (Confidence: 0.31)\n",
        "- {'Eggs'} -> {'Cereal'} (Confidence: 0.33)\n",
        "- {'Bread'} -> {'Tomato'} (Confidence: 0.36)\n",
        "- {'Tomato'} -> {'Bread'} (Confidence: 0.31)\n",
        "- {'Tomato'} -> {'Juice'} (Confidence: 0.32)\n",
        "- {'Juice'} -> {'Tomato'} (Confidence: 0.32)\n",
        "- {'Yogurt'} -> {'Tomato'} (Confidence: 0.31)\n",
        "- {'Fish'} -> {'Yogurt'} (Confidence: 0.33)\n",
        "- {'Yogurt'} -> {'Fish'} (Confidence: 0.31)\n",
        "- {'Butter'} -> {'Tomato'} (Confidence: 0.35)\n",
        "- {'Tomato'} -> {'Butter'} (Confidence: 0.32)\n",
        "- {'Yogurt'} -> {'Cereal'} (Confidence: 0.31)\n",
        "- {'Yogurt'} -> {'Pasta'} (Confidence: 0.34)\n",
        "- {'Pasta'} -> {'Yogurt'} (Confidence: 0.34)\n",
        "\n",
        "How Frequent Itemsets Change with Larger Datasets\n",
        "- Diversity Effect: Larger datasets contain a wider variety of items, making it harder for any single item to reach high support.\n",
        "- Multi-Item Patterns Are Rare: Since customers buy different combinations of products, multi-item frequent sets are significantly less common than single items. This explains why only at very low support levels (0.05) do 2-item frequent sets emerge.\n",
        "- Threshold Sensitivity: A high support threshold (e.g., 0.4) might be too restrictive, filtering out potential patterns. Lowering the threshold reveals more insights but also increases the risk of noise.\n",
        "\n",
        "Impact of Modifying Support & Confidence Thresholds\n",
        "Some rules might be weak, but they highlight possible relationships that a strict threshold would miss. Choosing the right support and confidence is a trade-off:\n",
        "- High thresholds → Fewer but stronger rules.\n",
        "- Low thresholds → More rules, but some may be weak or random.\n",
        "\n",
        " Certain items may not have strong one-to-one relationships but could be part of a larger pattern (e.g., clusters of products bought together).\n",
        "\n",
        " Practical Use Case: Retailers can use these insights to adjust promotions, product placements, and recommendations based on frequently co-purchased items.\n",
        "\n",
        "\n",
        " 3. Analyze which rules have the highest confidence and explain why they might be useful in business applications.\n",
        "\n",
        "- {'Bread'} → {'Tomato'} (Confidence: 0.36)\n",
        "- {'Butter'} → {'Tomato'} (Confidence: 0.35)\n",
        "- {'Yogurt'} → {'Pasta'} (Confidence: 0.34)\n",
        "- {'Pasta'} → {'Yogurt'} (Confidence: 0.34)\n",
        "Interpretation: If a customer buys bread, there is a 36% chance they will also buy tomatoes.\n",
        "\n",
        "Cross-Selling: Grocery stores can items near each other to encourage impulse purchases.\n",
        "\n",
        "Recipe-Based Bundling/ Targeted Promotions & Discounts: Bread and tomatoes are common ingredients for sandwiches, bruschetta, or toast. Promotions like \"Buy Bread & Get Tomatoes 10% Off\" could increase sales. Offer discounts on tomatoes when purchasing bread or butter.\n",
        "\n",
        "Better Customer Understanding: Recognizing hidden consumption patterns helps businesses tailor recommendations and stock products efficiently.\n",
        "\n",
        "\n",
        " 4. Discuss real-world applications of association rule mining beyond market basket analysis.\n",
        "- health care diagnosis\n",
        "- fraud detection\n",
        "- recommender systems (netflix, amazon)"
      ],
      "metadata": {
        "id": "3S6C_beM7Yul"
      }
    }
  ]
}